---
title: "vignettes"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{vignettes}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}
library(lineRegOLS)
```

To use the function `linear_regression_ols`, we must have sample data to test. Below, I've simulated random sample data for exploratory purposes. The data is complete nonsense, but assume that it represents people's score on a test, and we want to determine if their age, typical performance status (high, medium, low), and gender has a statistics. 

```{r Simulated Data}
set.seed(202311)
score = rnorm(1000, mean = 10, sd = 3)

# Covariates
age = rnorm(1000, mean = 35, sd = 7.5)

perf = round(runif(1000, 0, 2))
perf[perf == 0] = "low"
perf[perf == 1] = "medium"
perf[perf == 2] = "high"

gender = round(runif(1000, 0, 1))
gender[gender == "0"] = "male"
gender[gender == "1"] = "female"

testGrades = data.frame(score, age, perf, gender)

```

To run the function, we specify the parameters, with y as the response variable and x the vector of covariates we want to regress. By default, all non-binary categorical variables are coded into dummy variables using reference cell coding with the first unique category used as the reference. \emph{If you have multiple non-binary categorical variables, do not specify a reference category.} If you want to use mean cell coding, change the cat_coding parameter. My design, the linear model will only do reference cell or mean cell coding. 

At present, the function also cannot handle interaction terms. That is currently in development. See Version 1.1.

```{r Running the function}
model = linear_regression_ols(data = testGrades, y = "score", 
                      x = c("age", "perf", "gender"), reference_cat =  "low")

model
```

Testing for equivalnce: 
```{r Equivalence Test}
testGrades$perf.f = factor(testGrades$perf, levels = c("medium", "high", "low"))
compModel = lm(score~age+gender+perf.f, data = testGrades)
compSummary = summary(compModel)

all.equal(compModel, model)
```
Aside from a different data structures, I see that the only numerical difference between my function with the standard linear regression summary is in the calculation of the adjusted first and third quartiles for the residuals.  

To compare my code to the standard in run-time: 
```{r Benchmarking}
library(rbenchmark)
benchmark(myCode = linear_regression_ols(data = testGrades, y = "score", 
                      x = c("age", "perf", "gender"), reference_cat = "low"), 
          compCode = {
            compModel = lm(score~age+gender+perf.f, data = testGrades)
            summary(compModel)
          }
          )

```
Clearly, from the results above, my code runs less efficiently from the standard code. 


